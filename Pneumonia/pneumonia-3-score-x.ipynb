{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport random\n\nimport os\nfrom os import path\n\nimport matplotlib.pyplot as plt\n\nimport glob\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-08-19T10:42:12.357794Z","iopub.execute_input":"2022-08-19T10:42:12.358478Z","iopub.status.idle":"2022-08-19T10:42:12.364173Z","shell.execute_reply.started":"2022-08-19T10:42:12.358441Z","shell.execute_reply":"2022-08-19T10:42:12.363109Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Loading images paths","metadata":{}},{"cell_type":"code","source":"def get_images_path_list(dir_path):\n    path_list = glob.glob(\"{0}/*.jpeg\".format(dir_path))\n    return path_list\n\n\nROOT_DIR = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n\ntrain_normal = get_images_path_list(path.join(ROOT_DIR, 'train', 'NORMAL'))\nprint(\"len(train_normal): \", len(train_normal))\ntrain_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'train', 'PNEUMONIA'))\nprint(\"len(train_pneumonia): \", len(train_pneumonia))\n#balance train\ntrain_normal = random.sample(train_normal, min(len(train_normal), len(train_pneumonia)))\ntrain_pneumonia = random.sample(train_pneumonia, min(len(train_normal), len(train_pneumonia)))\nprint(\"len(train_normal) balanced: \", len(train_normal))\nprint(\"len(train_pneumonia) balanced: \", len(train_pneumonia))\n\ntest_normal = get_images_path_list(path.join(ROOT_DIR, 'test', 'NORMAL'))\nprint(\"len(test_normal): \", len(test_normal))\ntest_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'test', 'PNEUMONIA'))\nprint(\"len(test_pneumonia): \", len(test_pneumonia))\n#balance test\ntest_normal = random.sample(test_normal, min(len(test_normal), len(test_pneumonia)))\ntest_pneumonia = random.sample(test_pneumonia, min(len(test_normal), len(test_pneumonia)))\nprint(\"len(test_normal) balanced: \", len(test_normal))\nprint(\"len(test_pneumonia) balanced: \", len(test_pneumonia))\n\nval_normal = get_images_path_list(path.join(ROOT_DIR, 'val', 'NORMAL'))\nprint(\"len(val_normal): \", len(val_normal))\nval_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'val', 'PNEUMONIA'))\nprint(\"len(val_pneumonia): \", len(val_normal))\n\nprint(\"========== balancing val set=========\")\n#16 val photos are only afew. trying to balance\nval_balancing_thresh = 1200\nval_normal = train_normal[val_balancing_thresh:]\nval_pneumonia = train_pneumonia[val_balancing_thresh:]\ntrain_normal = train_normal[:val_balancing_thresh]\ntrain_pneumonia = train_pneumonia[:val_balancing_thresh]\nprint(\"len(train_normal) balanced: \", len(train_normal))\nprint(\"len(train_pneumonia) balanced: \", len(train_pneumonia))\nprint(\"len(test_normal) balanced: \", len(test_normal))\nprint(\"len(test_pneumonia) balanced: \", len(test_pneumonia))\nprint(\"len(val_normal): \", len(val_normal))\nprint(\"len(val_pneumonia): \", len(val_normal))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:35:05.264354Z","iopub.execute_input":"2022-08-19T11:35:05.265353Z","iopub.status.idle":"2022-08-19T11:35:05.306292Z","shell.execute_reply.started":"2022-08-19T11:35:05.265308Z","shell.execute_reply":"2022-08-19T11:35:05.305319Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# Importing TensorFlow and Keras","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input","metadata":{"execution":{"iopub.status.busy":"2022-08-19T10:42:14.855816Z","iopub.execute_input":"2022-08-19T10:42:14.856967Z","iopub.status.idle":"2022-08-19T10:42:20.123756Z","shell.execute_reply.started":"2022-08-19T10:42:14.856915Z","shell.execute_reply":"2022-08-19T10:42:20.122585Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"# Source https://www.tensorflow.org/tutorials/load_data/tfrecord\n\n# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef _image_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n    )\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T10:42:20.128286Z","iopub.execute_input":"2022-08-19T10:42:20.128805Z","iopub.status.idle":"2022-08-19T10:42:20.139369Z","shell.execute_reply.started":"2022-08-19T10:42:20.128775Z","shell.execute_reply":"2022-08-19T10:42:20.136990Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Creating TFRecord","metadata":{}},{"cell_type":"code","source":"tfrecords_dir = \"tfrecords_dir\"\n\nif not os.path.exists(tfrecords_dir):\n    os.makedirs(tfrecords_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save TFRecords\".format(tfrecords_dir))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T10:42:20.142820Z","iopub.execute_input":"2022-08-19T10:42:20.143930Z","iopub.status.idle":"2022-08-19T10:42:20.153234Z","shell.execute_reply.started":"2022-08-19T10:42:20.143873Z","shell.execute_reply":"2022-08-19T10:42:20.151892Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_example(image, label):\n    label = 0 if label == \"normal\" else 1\n    feature = { 'label': _int64_feature(label),\n              'image': _image_feature(image) }\n    return tf.train.Example(features=tf.train.Features(feature=feature))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T10:42:20.156685Z","iopub.execute_input":"2022-08-19T10:42:20.157162Z","iopub.status.idle":"2022-08-19T10:42:20.164253Z","shell.execute_reply.started":"2022-08-19T10:42:20.157095Z","shell.execute_reply":"2022-08-19T10:42:20.163184Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#serialize images from set (train, test, val) with their labels to a TFRecord file.\ndef write_serialized(imgs_labels, set_name):\n    # Initiating the writer and creating the tfrecord file.\n    tfrecord_filename = path.join(tfrecords_dir, \"file_{0}.tfrec\".format(set_name))\n    total_written = 0\n    with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n        for group in imgs_labels:# each group represents a class\n            imgs_paths = group[0]\n            label = group[1]\n            for i in range(len(imgs_paths)):\n                img_path = imgs_paths[i]\n                img = tf.io.decode_jpeg(tf.io.read_file(img_path))\n                    \n                #create example from features\n                example = create_example(img, label)\n                #write example to tfrecord\n                writer.write(example.SerializeToString())\n                total_written += 1\n                \n    print(\"Written {0} examples to {1}\".format(total_written, tfrecord_filename))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:35:23.133733Z","iopub.execute_input":"2022-08-19T11:35:23.134111Z","iopub.status.idle":"2022-08-19T11:35:23.141746Z","shell.execute_reply.started":"2022-08-19T11:35:23.134081Z","shell.execute_reply":"2022-08-19T11:35:23.140480Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"#Write train data to TFRecord\nimgs_labels = [\n    [train_pneumonia, \"pneumonia\"],\n    [train_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"train\")\n\n#Write test data to tf_record\nimgs_labels = [\n    [test_pneumonia, \"pneumonia\"],\n    [test_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"test\")\n\n#Write val data to tf_record\nimgs_labels = [\n    [val_pneumonia, \"pneumonia\"],\n    [val_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"val\")","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:35:27.539319Z","iopub.execute_input":"2022-08-19T11:35:27.540271Z","iopub.status.idle":"2022-08-19T11:36:44.857360Z","shell.execute_reply.started":"2022-08-19T11:35:27.540233Z","shell.execute_reply":"2022-08-19T11:36:44.856141Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#parse example [image + label] from TFRecord\ndef parse_tfrecord_fn(example):\n    feature_description = {\n        \"label\": tf.io.FixedLenFeature([], tf.int64),\n        \"image\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n    return example","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:38:19.573069Z","iopub.execute_input":"2022-08-19T11:38:19.573541Z","iopub.status.idle":"2022-08-19T11:38:19.585769Z","shell.execute_reply.started":"2022-08-19T11:38:19.573498Z","shell.execute_reply":"2022-08-19T11:38:19.584793Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#Explore one sample from the generated TFRecord\nraw_dataset = tf.data.TFRecordDataset(path.join(tfrecords_dir, \"file_train.tfrec\"))\nprint(\"Dataset size: \", len(list(raw_dataset)))\nparsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n\n\nfor features in parsed_dataset.take(1):\n    print(\"label: {0}\".format(\"Normal\" if features[\"label\"] == 0 else \"Pneumonia\"))\n    print(f\"Image shape: {features['image'].shape}\")\n    plt.figure(figsize=(7, 7))\n    plt.imshow(features[\"image\"].numpy())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:38:20.354360Z","iopub.execute_input":"2022-08-19T11:38:20.354790Z","iopub.status.idle":"2022-08-19T11:38:21.631280Z","shell.execute_reply.started":"2022-08-19T11:38:20.354750Z","shell.execute_reply":"2022-08-19T11:38:21.629922Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Train a simple model using the generated TFRecords","metadata":{}},{"cell_type":"code","source":"#data augmentation. Note : must only be applied on training set\ndata_augmentation = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip(\"horizontal\"),#or vertical or horizontal_and_vertical\n  tf.keras.layers.RandomRotation(0.2),\n  tf.keras.layers.RandomZoom(0.1),\n])\n\n#resize image to the correct input shape expected by the ML model\ndef prepare_sample(features):\n    image = tf.image.resize(features[\"image\"], size=(224, 224))\n    return image, features[\"label\"]\n\n\ndef get_dataset(filenames, batch_size, augment_data = False):\n    \n    dataset = (#create TFRecordDataset from images of passed filenames\n        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE) # parse TFRecord from each\n        .map(prepare_sample, num_parallel_calls=AUTOTUNE) # preprocessing the image in each TFRecord\n        .shuffle(batch_size * 10) # shuffle dataset. THIS IS VERY IMPORTANT TO AVOID OVERFITTING and RANDOM MODEL\n        .batch(batch_size) # get requested batch size\n        .prefetch(AUTOTUNE) # Make sure that there is always 1 batch ready for the GPU prefetched by the CPU\n        # batch prefetch Stackoverflow https://stackoverflow.com/a/67361329/11292753\n    )\n    \n    #will only augment train data for better training and to avoid overfitting\n    if augment_data:\n        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n        \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:38:52.142083Z","iopub.execute_input":"2022-08-19T11:38:52.142451Z","iopub.status.idle":"2022-08-19T11:38:52.162682Z","shell.execute_reply.started":"2022-08-19T11:38:52.142420Z","shell.execute_reply":"2022-08-19T11:38:52.161565Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# add PretrainedModel enumeration to make it easier to pick supported pretrained_models\nfrom enum import Enum\nclass PretrainedModel(Enum):\n    VGG16=1,\n    VGG19=2,\n    Xception=3","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:39:38.325207Z","iopub.execute_input":"2022-08-19T11:39:38.325575Z","iopub.status.idle":"2022-08-19T11:39:38.331698Z","shell.execute_reply.started":"2022-08-19T11:39:38.325543Z","shell.execute_reply":"2022-08-19T11:39:38.330523Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# get pretrained VGG16 model using imagenet weights\ndef load_vgg16_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    \n    # consider which layers you want to freeze before you compile \n    for i in range(17):\n        base_model.layers[i].trainable = False\n        \n    return base_model\n\n# get pretrained VGG19 model using imagenet weights\ndef load_vgg19_pretrained_model(input_shape):\n    base_model = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n        \n    # consider which layers you want to freeze before you compile \n    for i in range(20):\n        base_model.layers[i].trainable = False\n        \n    return base_model\n        \n# get pretrained Xception model using imagenet weights\ndef load_xception_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.Xception(weights='imagenet',\n                                                include_top=False,\n                                                input_shape=input_shape)\n\n    # freeze all layers but last 10\n    for i in range(len(base_model.layers) - 10):\n         base_model.layers[i].trainable = False\n\n    return base_model\n\n#build our model based on pretrained model\ndef make_model(pretrained_model, exponential_decay=False, print_summary = False):\n    \n    input_shape = (224, 224, 3)\n    \n    if pretrained_model == PretrainedModel.VGG16:\n        base_model = load_vgg16_pretrained_model(input_shape)\n    elif pretrained_model == PretrainedModel.VGG19:\n        base_model = load_vgg19_pretrained_model(input_shape)\n    elif pretrained_model == PretrainedModel.Xception:\n        base_model = load_xception_pretrained_model(input_shape)\n    else:\n        raise Exception(\"Unsupported pretrained model.\")\n\n    x = base_model.output\n    x = Flatten()(x) #flatten all the output to FC layer\n    x = Dense(512, activation='relu')(x) # Relu Dense of 512 units\n    x = Dropout(0.3)(x) # use Dropout to decrease overfitting\n    x = Dense(256, activation='relu')(x)\n    predictions = Dense(1, activation='sigmoid')(x) # Sigmoid activation: [0,1]\n    model = Model(inputs=base_model.inputs, outputs=predictions)\n        \n    #use exponential decay to have different learning weights for trainable layers from\n    # the pretrained model and ones we added on top.\n    initial_learning_rate = 0.01\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=10, decay_rate=0.96, staircase=True\n    )\n\n    loss = 'binary_crossentropy'  #either pneumonia or normal\n    metrics = ['binary_accuracy']\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5 if not exponential_decay else lr_schedule),\n        loss=loss,\n        metrics=metrics,\n    )\n    \n    if print_summary:\n        model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:39:47.663016Z","iopub.execute_input":"2022-08-19T11:39:47.663393Z","iopub.status.idle":"2022-08-19T11:39:47.677192Z","shell.execute_reply.started":"2022-08-19T11:39:47.663362Z","shell.execute_reply":"2022-08-19T11:39:47.675829Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"#create checkboint and earlystopping to use by history\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    \"pneumonia_model.h5\", save_best_only=True, monitor= 'val_loss'\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    patience=1000, restore_best_weights=True, monitor= 'val_loss'\n)\n\nmodel = make_model(PretrainedModel.Xception, exponential_decay = True, print_summary=False)\nprint(\"Build successfully!\")","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:40:08.977130Z","iopub.execute_input":"2022-08-19T11:40:08.977525Z","iopub.status.idle":"2022-08-19T11:40:10.293887Z","shell.execute_reply.started":"2022-08-19T11:40:08.977495Z","shell.execute_reply":"2022-08-19T11:40:10.292780Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"# Train our model","metadata":{}},{"cell_type":"code","source":"train_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_train.tfrec\")\nval_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_val.tfrec\")\n\nbatch_size = 64\nepochs = 20\nn_train_images = len(train_normal) + len(train_pneumonia)\nsteps_per_epoch = 10\n\n# From Stackover: tf.data builds a performance model of the input pipeline\n# and runs an optimization algorithm to find a good allocation of its CPU \n# budget across all parameters specified as AUTOTUNE\n# Stackover flow https://stackoverflow.com/a/59493168/11292753\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = get_dataset(train_filenames, batch_size, augment_data=True) #augment only train_set!!\nval_dataset = get_dataset(val_filenames, 16)\n\n# debugging useful\n# image_batch = next(iter(train_dataset))\n# imgs_tensor = tf.constant(image_batch[0])\n# labels_tensor=tf.constant(image_batch[1])\n\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:40:13.575888Z","iopub.execute_input":"2022-08-19T11:40:13.577161Z","iopub.status.idle":"2022-08-19T11:41:38.279283Z","shell.execute_reply.started":"2022-08-19T11:40:13.577109Z","shell.execute_reply":"2022-08-19T11:41:38.278230Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def plot_history_acc(history, loss_metric_type):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    if loss_metric_type == \"binary\":\n        plt.plot(np.arange(0, N), history.history[\"binary_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_binary_accuracy\"], label=\"val_acc\")\n    else:\n        plt.plot(np.arange(0, N), history.history[\"categorical_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_categorical_accuracy\"], label=\"val_acc\")\n        \n    plt.title(\"Training Accuracy on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"lower left\")\n    \n    return\n\ndef plot_history_loss(history):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n        \n    plt.title(\"Training Loss on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"lower left\")\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:41:38.282014Z","iopub.execute_input":"2022-08-19T11:41:38.282381Z","iopub.status.idle":"2022-08-19T11:41:38.292814Z","shell.execute_reply.started":"2022-08-19T11:41:38.282345Z","shell.execute_reply":"2022-08-19T11:41:38.291547Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"#plot epoch-accuracy chart\nloss_metric_type = 'binary' #'binary'\nplot_history_acc(history, loss_metric_type)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:41:38.294243Z","iopub.execute_input":"2022-08-19T11:41:38.294589Z","iopub.status.idle":"2022-08-19T11:41:38.506757Z","shell.execute_reply.started":"2022-08-19T11:41:38.294555Z","shell.execute_reply":"2022-08-19T11:41:38.505842Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#plot epoch-loss chart\nplot_history_loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:41:38.508844Z","iopub.execute_input":"2022-08-19T11:41:38.509321Z","iopub.status.idle":"2022-08-19T11:41:38.715818Z","shell.execute_reply.started":"2022-08-19T11:41:38.509284Z","shell.execute_reply":"2022-08-19T11:41:38.714824Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model","metadata":{}},{"cell_type":"code","source":"test_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_test.tfrec\")\n\nn_test_images = len(train_normal) + len(train_pneumonia)\ntest_dataset = get_dataset(test_filenames, batch_size)\n\nmodel.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:46:01.817810Z","iopub.execute_input":"2022-08-19T11:46:01.818305Z","iopub.status.idle":"2022-08-19T11:46:05.748233Z","shell.execute_reply.started":"2022-08-19T11:46:01.818253Z","shell.execute_reply":"2022-08-19T11:46:05.746946Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def show_batch_predictions(image_batch):\n    \n    #get rid of Eager Tensor to be able to iterate\n    img_tensor=tf.constant(image_batch[0])\n    labels_tensor=tf.constant(image_batch[1])\n    \n    # debugging: print correct labels: 1= pneumonia, 0 = normal\n    print(\"correct labels:\", labels_tensor)\n    \n    plt.figure(figsize=(14, 20))\n    for n in range(batch_size):\n        ax = plt.subplot(16, 4, n + 1)\n        plt.imshow(img_tensor[n] / 255.0)\n        img_array = tf.expand_dims(img_tensor[n], axis=0)\n        #print( model.predict(img_array)[0])\n        num_predicted_label = model.predict(img_array)[0]\n        \n        # debugging\n        #print(num_predicted_label)\n        \n        # from TF docs:\n        # Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero.\n        # The sigmoid function always returns a value between 0 and 1.\n\n        str_label = \"normal\" if num_predicted_label < 0.5 else \"pneumonia\"\n        correct = \"T\" if num_predicted_label == labels_tensor[n] else \"F\"\n        plt.title(\"{0} - {1}\".format(str_label, correct))\n        plt.axis(\"off\")\n\n\nimage_batch = next(iter(test_dataset))\nshow_batch_predictions(image_batch)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T11:45:11.373371Z","iopub.execute_input":"2022-08-19T11:45:11.373754Z","iopub.status.idle":"2022-08-19T11:45:20.001160Z","shell.execute_reply.started":"2022-08-19T11:45:11.373720Z","shell.execute_reply":"2022-08-19T11:45:20.000291Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"export_dir= \"export_dir\"\n\nif not os.path.exists(export_dir):\n    os.makedirs(export_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save exported model\".format(export_dir))\n\nexport_path = path.join(\"export_dir\", \"pneumonia.pb\")\n\ntf.keras.models.save_model(\n    model,\n    export_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None\n)\n\nprint('\\nSaved model')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T08:23:16.206516Z","iopub.status.idle":"2022-08-19T08:23:16.207373Z","shell.execute_reply.started":"2022-08-19T08:23:16.207119Z","shell.execute_reply":"2022-08-19T08:23:16.207143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download model","metadata":{}},{"cell_type":"code","source":"# from IPython.display import FileLink\n\n# !zip -r model.zip export_dir\n# FileLink(r'model.zip')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T08:23:16.208762Z","iopub.status.idle":"2022-08-19T08:23:16.209866Z","shell.execute_reply.started":"2022-08-19T08:23:16.209589Z","shell.execute_reply":"2022-08-19T08:23:16.209613Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
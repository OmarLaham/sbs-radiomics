{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport random\n\nimport os\nfrom os import path\n\nimport matplotlib.pyplot as plt\n\nimport glob\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:31:16.470183Z","iopub.execute_input":"2022-08-17T09:31:16.470754Z","iopub.status.idle":"2022-08-17T09:31:16.500877Z","shell.execute_reply.started":"2022-08-17T09:31:16.470637Z","shell.execute_reply":"2022-08-17T09:31:16.499781Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Loading images paths","metadata":{}},{"cell_type":"code","source":"def get_images_path_list(dir_path):\n    path_list = glob.glob(\"{0}/*.jpeg\".format(dir_path))\n    return path_list\n\n\nROOT_DIR = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n\ntrain_normal = get_images_path_list(path.join(ROOT_DIR, 'train', 'NORMAL'))\nprint(\"len(train_normal): \", len(train_normal))\ntrain_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'train', 'PNEUMONIA'))\nprint(\"len(train_pneumonia): \", len(train_pneumonia))\n#balance train\ntrain_normal = random.sample(train_normal, min(len(train_normal), len(train_pneumonia)))\ntrain_pneumonia = random.sample(train_pneumonia, min(len(train_normal), len(train_pneumonia)))\nprint(\"len(train_normal) balanced: \", len(train_normal))\nprint(\"len(train_pneumonia) balanced: \", len(train_pneumonia))\n\ntest_normal = get_images_path_list(path.join(ROOT_DIR, 'test', 'NORMAL'))\nprint(\"len(test_normal): \", len(test_normal))\ntest_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'test', 'PNEUMONIA'))\nprint(\"len(test_pneumonia): \", len(test_pneumonia))\n#balance test\ntest_normal = random.sample(test_normal, min(len(test_normal), len(test_pneumonia)))\ntest_pneumonia = random.sample(test_pneumonia, min(len(test_normal), len(test_pneumonia)))\nprint(\"len(test_normal) balanced: \", len(test_normal))\nprint(\"len(test_pneumonia) balanced: \", len(test_pneumonia))\n\nval_normal = get_images_path_list(path.join(ROOT_DIR, 'val', 'NORMAL'))\nprint(\"len(val_normal): \", len(val_normal))\nval_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'val', 'PNEUMONIA'))\nprint(\"len(val_pneumonia): \", len(val_normal))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:32:05.115247Z","iopub.execute_input":"2022-08-17T09:32:05.115691Z","iopub.status.idle":"2022-08-17T09:32:05.855436Z","shell.execute_reply.started":"2022-08-17T09:32:05.115650Z","shell.execute_reply":"2022-08-17T09:32:05.854161Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing TensorFlow and Keras","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:32:50.603408Z","iopub.execute_input":"2022-08-17T09:32:50.603840Z","iopub.status.idle":"2022-08-17T09:32:59.915047Z","shell.execute_reply.started":"2022-08-17T09:32:50.603806Z","shell.execute_reply":"2022-08-17T09:32:59.913721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"# Source https://www.tensorflow.org/tutorials/load_data/tfrecord\n\n# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef _image_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n    )\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:32:59.919311Z","iopub.execute_input":"2022-08-17T09:32:59.920643Z","iopub.status.idle":"2022-08-17T09:32:59.929488Z","shell.execute_reply.started":"2022-08-17T09:32:59.920593Z","shell.execute_reply":"2022-08-17T09:32:59.928253Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Creating TFRecord","metadata":{}},{"cell_type":"code","source":"tfrecords_dir = \"tfrecords_dir\"\n\nif not os.path.exists(tfrecords_dir):\n    os.makedirs(tfrecords_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save TFRecords\".format(tfrecords_dir))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:33:03.158464Z","iopub.execute_input":"2022-08-17T09:33:03.158888Z","iopub.status.idle":"2022-08-17T09:33:03.167916Z","shell.execute_reply.started":"2022-08-17T09:33:03.158850Z","shell.execute_reply":"2022-08-17T09:33:03.165359Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_example(image, label):\n    label = 0 if label == \"normal\" else 1\n    feature = { 'label': _int64_feature(label),\n              'image': _image_feature(image) }\n    return tf.train.Example(features=tf.train.Features(feature=feature))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:33:05.679601Z","iopub.execute_input":"2022-08-17T09:33:05.680473Z","iopub.status.idle":"2022-08-17T09:33:05.687614Z","shell.execute_reply.started":"2022-08-17T09:33:05.680419Z","shell.execute_reply":"2022-08-17T09:33:05.686514Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#serialize images from set (train, test, val) with their labels to a TFRecord file.\ndef write_serialized(imgs_labels, set_name):\n    # Initiating the writer and creating the tfrecord file.\n    tfrecord_filename = path.join(tfrecords_dir, \"file_{0}.tfrec\".format(set_name))\n    total_written = 0\n    with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n        for group in imgs_labels:# each group represents a class\n            imgs_paths = group[0]\n            label = group[1]\n            for i in range(len(imgs_paths)):\n                img_path = imgs_paths[i]\n                img = tf.io.decode_jpeg(tf.io.read_file(img_path))\n\n                #create example from features\n                example = create_example(img, label)\n                #write example to tfrecord\n                writer.write(example.SerializeToString())\n                total_written += 1\n                \n    print(\"Written {0} examples to {1}\".format(total_written, tfrecord_filename))","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:34:59.634486Z","iopub.execute_input":"2022-08-17T09:34:59.634914Z","iopub.status.idle":"2022-08-17T09:34:59.643805Z","shell.execute_reply.started":"2022-08-17T09:34:59.634876Z","shell.execute_reply":"2022-08-17T09:34:59.642579Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Write train data to TFRecord\nimgs_labels = [\n    [train_pneumonia, \"pneumonia\"],\n    [train_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"train\")\n\n#Write test data to tf_record\nimgs_labels = [\n    [test_pneumonia, \"pneumonia\"],\n    [test_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"test\")\n\n#Write val data to tf_record\nimgs_labels = [\n    [val_pneumonia, \"pneumonia\"],\n    [val_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"val\")","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:35:01.437516Z","iopub.execute_input":"2022-08-17T09:35:01.437979Z","iopub.status.idle":"2022-08-17T09:36:26.779009Z","shell.execute_reply.started":"2022-08-17T09:35:01.437938Z","shell.execute_reply":"2022-08-17T09:36:26.777625Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#parse example [image + label] from TFRecord\ndef parse_tfrecord_fn(example):\n    feature_description = {\n        \"label\": tf.io.FixedLenFeature([], tf.int64),\n        \"image\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n    return example","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:36:39.277507Z","iopub.execute_input":"2022-08-17T09:36:39.278347Z","iopub.status.idle":"2022-08-17T09:36:39.285370Z","shell.execute_reply.started":"2022-08-17T09:36:39.278305Z","shell.execute_reply":"2022-08-17T09:36:39.284056Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Explore one sample from the generated TFRecord\nraw_dataset = tf.data.TFRecordDataset(path.join(tfrecords_dir, \"file_train.tfrec\"))\nprint(\"Dataset size: \", len(list(raw_dataset)))\nparsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n\n\nfor features in parsed_dataset.take(1):\n    print(\"label: {0}\".format(\"Normal\" if features[\"label\"] == 0 else \"Pneumonia\"))\n    print(f\"Image shape: {features['image'].shape}\")\n    plt.figure(figsize=(7, 7))\n    plt.imshow(features[\"image\"].numpy())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T09:36:44.035936Z","iopub.execute_input":"2022-08-17T09:36:44.037130Z","iopub.status.idle":"2022-08-17T09:36:50.073775Z","shell.execute_reply.started":"2022-08-17T09:36:44.037087Z","shell.execute_reply":"2022-08-17T09:36:50.072597Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Train a simple model using the generated TFRecords","metadata":{}},{"cell_type":"code","source":"#resize image to the correct input shape expected by the ML model\ndef prepare_sample(features):\n    image = tf.image.resize(features[\"image\"], size=(224, 224))\n    return image, features[\"label\"]\n\n\ndef get_dataset(filenames, batch_size):\n    dataset = (#create TFRecordDataset from images of passed filenames\n        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE) # parse TFRecord from each\n        .map(prepare_sample, num_parallel_calls=AUTOTUNE) # preprocessing the image in each TFRecord\n        .shuffle(batch_size * 100) # shuffle dataset. THIS IS VERY IMPORTANT TO AVOID OVERFITTING and RANDOM MODEL\n        .batch(batch_size) # get requested batch size\n        .prefetch(AUTOTUNE) # Make sure that there is always 1 batch ready for the GPU prefetched by the CPU\n        # batch prefetch Stackoverflow https://stackoverflow.com/a/67361329/11292753\n    )\n    return dataset\n\n# get pretrained VGG16 model using imagenet weights\ndef load_vgg16_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.VGG16(weights='imagenet', input_shape=input_shape)\n    transfer_layer = base_model.get_layer('block5_pool')\n    pretrained_model = Model(inputs = base_model.input, outputs = transfer_layer.output)\n    \n    # consider which layers you want to freeze before you compile \n    for i in range(17):\n        pretrained_model.layers[i].trainable = False\n\n# get pretrained Xception model using imagenet weights\ndef load_xception_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.Xception(weights='imagenet',\n                                                include_top=False,\n                                                input_shape=input_shape)\n    \n    \n    # freeze all layers but last 10\n    for i in range(len(base_model.layers) - 10):\n         base_model.layers[i].trainable = False\n    return base_model\n\n#build our model based on pretrained model\ndef make_model(exponential_decay, print_summary = False):\n    \n    input_shape = (224, 224, 3)\n\n    base_model = load_xception_pretrained_model(input_shape)\n    x = base_model.output\n    x = Flatten()(x) #flatten all the output to FC layer\n    x = Dense(512, activation='relu')(x) # Relu Dense of 512 units\n    x = Dropout(0.3)(x) # use Dropout to decrease overfitting\n    x = Dense(256, activation='relu')(x)\n    predictions = Dense(1, activation='sigmoid')(x) # Sigmoid activation: [0,1]\n    model = Model(inputs=base_model.inputs, outputs=predictions)\n        \n    #use exponential decay to have different learning weights for trainable layers from\n    # the pretrained model and ones we added on top.\n    initial_learning_rate = 0.01\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n    )\n\n    loss = 'binary_crossentropy'  #either pneumonia or normal\n    metrics = ['binary_accuracy']\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5 if not exponential_decay else lr_schedule),\n        loss=loss,\n        metrics=metrics,\n    )\n    \n    if print_summary:\n        model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:53:40.238816Z","iopub.execute_input":"2022-08-17T10:53:40.239258Z","iopub.status.idle":"2022-08-17T10:53:40.256633Z","shell.execute_reply.started":"2022-08-17T10:53:40.239221Z","shell.execute_reply":"2022-08-17T10:53:40.255115Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#create checkboint and earlystopping to use by history\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    \"pneumonia_model.h5\", save_best_only=True, monitor= 'val_loss'\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    patience=100, restore_best_weights=True, monitor= 'val_loss'\n)\n\nmodel = make_model(exponential_decay = True, print_summary=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:53:40.819226Z","iopub.execute_input":"2022-08-17T10:53:40.819698Z","iopub.status.idle":"2022-08-17T10:53:42.488415Z","shell.execute_reply.started":"2022-08-17T10:53:40.819653Z","shell.execute_reply":"2022-08-17T10:53:42.487118Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Train our model","metadata":{}},{"cell_type":"code","source":"train_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_train.tfrec\")\nval_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_val.tfrec\")\n\nbatch_size = 32\nepochs = 50\nsteps_per_epoch = 15\n\n# From Stackover: tf.data builds a performance model of the input pipeline\n# and runs an optimization algorithm to find a good allocation of its CPU \n# budget across all parameters specified as AUTOTUNE\n# Stackover flow https://stackoverflow.com/a/59493168/11292753\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = get_dataset(train_filenames, batch_size)\nval_dataset = get_dataset(val_filenames, 8)\n\n# debugging useful\n# image_batch = next(iter(train_dataset))\n# imgs_tensor = tf.constant(image_batch[0])\n# labels_tensor=tf.constant(image_batch[1])\n\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    #callbacks=[checkpoint_cb, early_stopping_cb],\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:53:47.006689Z","iopub.execute_input":"2022-08-17T10:53:47.008955Z","iopub.status.idle":"2022-08-17T10:58:15.218800Z","shell.execute_reply.started":"2022-08-17T10:53:47.008892Z","shell.execute_reply":"2022-08-17T10:58:15.217646Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def plot_history_acc(history, loss_metric_type):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    if loss_metric_type == \"binary\":\n        plt.plot(np.arange(0, N), history.history[\"binary_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_binary_accuracy\"], label=\"val_acc\")\n    else:\n        plt.plot(np.arange(0, N), history.history[\"categorical_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_categorical_accuracy\"], label=\"val_acc\")\n        \n    plt.title(\"Training Accuracy on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"lower left\")\n    \n    return\n\ndef plot_history_loss(history):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n        \n    plt.title(\"Training Loss on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"lower left\")\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:58:17.872777Z","iopub.execute_input":"2022-08-17T10:58:17.873589Z","iopub.status.idle":"2022-08-17T10:58:17.885180Z","shell.execute_reply.started":"2022-08-17T10:58:17.873533Z","shell.execute_reply":"2022-08-17T10:58:17.883838Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#plot epoch-accuracy chart\nloss_metric_type = 'binary' #'binary'\nplot_history_acc(history, loss_metric_type)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:58:18.233489Z","iopub.execute_input":"2022-08-17T10:58:18.233982Z","iopub.status.idle":"2022-08-17T10:58:18.409857Z","shell.execute_reply.started":"2022-08-17T10:58:18.233941Z","shell.execute_reply":"2022-08-17T10:58:18.408570Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#plot epoch-loss chart\nplot_history_loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T10:58:18.751523Z","iopub.execute_input":"2022-08-17T10:58:18.751979Z","iopub.status.idle":"2022-08-17T10:58:18.909110Z","shell.execute_reply.started":"2022-08-17T10:58:18.751938Z","shell.execute_reply":"2022-08-17T10:58:18.908093Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model","metadata":{}},{"cell_type":"code","source":"test_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_test.tfrec\")\ntest_dataset = get_dataset(test_filenames, batch_size)\n\nmodel.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T11:07:33.682043Z","iopub.execute_input":"2022-08-17T11:07:33.682504Z","iopub.status.idle":"2022-08-17T11:08:09.313263Z","shell.execute_reply.started":"2022-08-17T11:07:33.682465Z","shell.execute_reply":"2022-08-17T11:08:09.312153Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def show_batch_predictions(image_batch):\n    \n    #get rid of Eager Tensor to be able to iterate\n    img_tensor=tf.constant(image_batch[0])\n    labels_tensor=tf.constant(image_batch[1])\n    \n    # debugging: print correct labels: 1= pneumonia, 0 = normal\n    print(\"correct labels:\", labels_tensor)\n    \n    plt.figure(figsize=(14, 14))\n    for n in range(32):\n        ax = plt.subplot(8, 4, n + 1)\n        plt.imshow(img_tensor[n] / 255.0)\n        img_array = tf.expand_dims(img_tensor[n], axis=0)\n        #print( model.predict(img_array)[0])\n        num_predicted_label = model.predict(img_array)[0]\n        \n        # debugging\n        #print(num_predicted_label)\n        \n        # from TF docs:\n        # Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero.\n        # The sigmoid function always returns a value between 0 and 1.\n\n        str_label = \"normal\" if num_predicted_label < 0.5 else \"pneumonia\"\n        correct = \"T\" if num_predicted_label == labels_tensor[n] else \"F\"\n        plt.title(\"{0} - {1}\".format(str_label, correct))\n        plt.axis(\"off\")\n\n\nimage_batch = next(iter(test_dataset))\nshow_batch_predictions(image_batch)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T11:03:21.895797Z","iopub.execute_input":"2022-08-17T11:03:21.896241Z","iopub.status.idle":"2022-08-17T11:03:31.377468Z","shell.execute_reply.started":"2022-08-17T11:03:21.896204Z","shell.execute_reply":"2022-08-17T11:03:31.376254Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"export_dir= \"export_dir\"\n\nif not os.path.exists(export_dir):\n    os.makedirs(export_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save exported model\".format(export_dir))\n\nexport_path = path.join(\"export_dir\", \"pneumonia.pb\")\n\ntf.keras.models.save_model(\n    model,\n    export_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None\n)\n\nprint('\\nSaved model')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T11:00:46.082509Z","iopub.execute_input":"2022-08-17T11:00:46.083893Z","iopub.status.idle":"2022-08-17T11:01:11.437997Z","shell.execute_reply.started":"2022-08-17T11:00:46.083834Z","shell.execute_reply":"2022-08-17T11:01:11.436719Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Download model","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\n!zip -r model.zip export_dir\nFileLink(r'model.zip')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T11:01:01.981520Z","iopub.execute_input":"2022-07-18T11:01:01.981981Z","iopub.status.idle":"2022-07-18T11:01:43.734486Z","shell.execute_reply.started":"2022-07-18T11:01:01.981946Z","shell.execute_reply":"2022-07-18T11:01:43.733332Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
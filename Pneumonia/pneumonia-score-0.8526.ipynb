{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport random\n\nimport os\nfrom os import path\n\nimport matplotlib.pyplot as plt\n\nimport glob\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:46.787856Z","iopub.execute_input":"2022-08-18T08:19:46.788371Z","iopub.status.idle":"2022-08-18T08:19:46.830406Z","shell.execute_reply.started":"2022-08-18T08:19:46.788256Z","shell.execute_reply":"2022-08-18T08:19:46.829409Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Loading images paths","metadata":{}},{"cell_type":"code","source":"def get_images_path_list(dir_path):\n    path_list = glob.glob(\"{0}/*.jpeg\".format(dir_path))\n    return path_list\n\n\nROOT_DIR = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray'\n\ntrain_normal = get_images_path_list(path.join(ROOT_DIR, 'train', 'NORMAL'))\nprint(\"len(train_normal): \", len(train_normal))\ntrain_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'train', 'PNEUMONIA'))\nprint(\"len(train_pneumonia): \", len(train_pneumonia))\n#balance train\ntrain_normal = random.sample(train_normal, min(len(train_normal), len(train_pneumonia)))\ntrain_pneumonia = random.sample(train_pneumonia, min(len(train_normal), len(train_pneumonia)))\nprint(\"len(train_normal) balanced: \", len(train_normal))\nprint(\"len(train_pneumonia) balanced: \", len(train_pneumonia))\n\ntest_normal = get_images_path_list(path.join(ROOT_DIR, 'test', 'NORMAL'))\nprint(\"len(test_normal): \", len(test_normal))\ntest_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'test', 'PNEUMONIA'))\nprint(\"len(test_pneumonia): \", len(test_pneumonia))\n#balance test\ntest_normal = random.sample(test_normal, min(len(test_normal), len(test_pneumonia)))\ntest_pneumonia = random.sample(test_pneumonia, min(len(test_normal), len(test_pneumonia)))\nprint(\"len(test_normal) balanced: \", len(test_normal))\nprint(\"len(test_pneumonia) balanced: \", len(test_pneumonia))\n\nval_normal = get_images_path_list(path.join(ROOT_DIR, 'val', 'NORMAL'))\nprint(\"len(val_normal): \", len(val_normal))\nval_pneumonia = get_images_path_list(path.join(ROOT_DIR, 'val', 'PNEUMONIA'))\nprint(\"len(val_pneumonia): \", len(val_normal))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:46.832452Z","iopub.execute_input":"2022-08-18T08:19:46.833117Z","iopub.status.idle":"2022-08-18T08:19:47.943741Z","shell.execute_reply.started":"2022-08-18T08:19:46.833081Z","shell.execute_reply":"2022-08-18T08:19:47.942726Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing TensorFlow and Keras","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:47.945589Z","iopub.execute_input":"2022-08-18T08:19:47.946274Z","iopub.status.idle":"2022-08-18T08:19:53.241879Z","shell.execute_reply.started":"2022-08-18T08:19:47.946235Z","shell.execute_reply":"2022-08-18T08:19:53.240905Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"# Source https://www.tensorflow.org/tutorials/load_data/tfrecord\n\n# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef _image_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n    )\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:53.244652Z","iopub.execute_input":"2022-08-18T08:19:53.245413Z","iopub.status.idle":"2022-08-18T08:19:53.253113Z","shell.execute_reply.started":"2022-08-18T08:19:53.245369Z","shell.execute_reply":"2022-08-18T08:19:53.252102Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Creating TFRecord","metadata":{}},{"cell_type":"code","source":"tfrecords_dir = \"tfrecords_dir\"\n\nif not os.path.exists(tfrecords_dir):\n    os.makedirs(tfrecords_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save TFRecords\".format(tfrecords_dir))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:53.254716Z","iopub.execute_input":"2022-08-18T08:19:53.255393Z","iopub.status.idle":"2022-08-18T08:19:53.270768Z","shell.execute_reply.started":"2022-08-18T08:19:53.255356Z","shell.execute_reply":"2022-08-18T08:19:53.269692Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_example(image, label):\n    label = 0 if label == \"normal\" else 1\n    feature = { 'label': _int64_feature(label),\n              'image': _image_feature(image) }\n    return tf.train.Example(features=tf.train.Features(feature=feature))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:53.272508Z","iopub.execute_input":"2022-08-18T08:19:53.272875Z","iopub.status.idle":"2022-08-18T08:19:53.279664Z","shell.execute_reply.started":"2022-08-18T08:19:53.272834Z","shell.execute_reply":"2022-08-18T08:19:53.278511Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#serialize images from set (train, test, val) with their labels to a TFRecord file.\ndef write_serialized(imgs_labels, set_name):\n    # Initiating the writer and creating the tfrecord file.\n    tfrecord_filename = path.join(tfrecords_dir, \"file_{0}.tfrec\".format(set_name))\n    total_written = 0\n    with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n        for group in imgs_labels:# each group represents a class\n            imgs_paths = group[0]\n            label = group[1]\n            for i in range(len(imgs_paths)):\n                img_path = imgs_paths[i]\n                img = tf.io.decode_jpeg(tf.io.read_file(img_path))\n\n                #create example from features\n                example = create_example(img, label)\n                #write example to tfrecord\n                writer.write(example.SerializeToString())\n                total_written += 1\n                \n    print(\"Written {0} examples to {1}\".format(total_written, tfrecord_filename))","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:53.281789Z","iopub.execute_input":"2022-08-18T08:19:53.282138Z","iopub.status.idle":"2022-08-18T08:19:53.292567Z","shell.execute_reply.started":"2022-08-18T08:19:53.282104Z","shell.execute_reply":"2022-08-18T08:19:53.291652Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Write train data to TFRecord\nimgs_labels = [\n    [train_pneumonia, \"pneumonia\"],\n    [train_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"train\")\n\n#Write test data to tf_record\nimgs_labels = [\n    [test_pneumonia, \"pneumonia\"],\n    [test_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"test\")\n\n#Write val data to tf_record\nimgs_labels = [\n    [val_pneumonia, \"pneumonia\"],\n    [val_normal, \"normal\"]\n]\nwrite_serialized(imgs_labels, \"val\")","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:19:53.293904Z","iopub.execute_input":"2022-08-18T08:19:53.294351Z","iopub.status.idle":"2022-08-18T08:21:24.179913Z","shell.execute_reply.started":"2022-08-18T08:19:53.294314Z","shell.execute_reply":"2022-08-18T08:21:24.178867Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#parse example [image + label] from TFRecord\ndef parse_tfrecord_fn(example):\n    feature_description = {\n        \"label\": tf.io.FixedLenFeature([], tf.int64),\n        \"image\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n    return example","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:21:24.181299Z","iopub.execute_input":"2022-08-18T08:21:24.181868Z","iopub.status.idle":"2022-08-18T08:21:24.189075Z","shell.execute_reply.started":"2022-08-18T08:21:24.181831Z","shell.execute_reply":"2022-08-18T08:21:24.187966Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Explore one sample from the generated TFRecord\nraw_dataset = tf.data.TFRecordDataset(path.join(tfrecords_dir, \"file_train.tfrec\"))\nprint(\"Dataset size: \", len(list(raw_dataset)))\nparsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n\n\nfor features in parsed_dataset.take(1):\n    print(\"label: {0}\".format(\"Normal\" if features[\"label\"] == 0 else \"Pneumonia\"))\n    print(f\"Image shape: {features['image'].shape}\")\n    plt.figure(figsize=(7, 7))\n    plt.imshow(features[\"image\"].numpy())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:21:24.194128Z","iopub.execute_input":"2022-08-18T08:21:24.195036Z","iopub.status.idle":"2022-08-18T08:21:26.523545Z","shell.execute_reply.started":"2022-08-18T08:21:24.194979Z","shell.execute_reply":"2022-08-18T08:21:26.522604Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Train a simple model using the generated TFRecords","metadata":{}},{"cell_type":"code","source":"#resize image to the correct input shape expected by the ML model\ndef prepare_sample(features):\n    image = tf.image.resize(features[\"image\"], size=(224, 224))\n    return image, features[\"label\"]\n\n\ndef get_dataset(filenames, batch_size):\n    dataset = (#create TFRecordDataset from images of passed filenames\n        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE) # parse TFRecord from each\n        .map(prepare_sample, num_parallel_calls=AUTOTUNE) # preprocessing the image in each TFRecord\n        .shuffle(batch_size * 10) # shuffle dataset. THIS IS VERY IMPORTANT TO AVOID OVERFITTING and RANDOM MODEL\n        .batch(batch_size) # get requested batch size\n        .prefetch(AUTOTUNE) # Make sure that there is always 1 batch ready for the GPU prefetched by the CPU\n        # batch prefetch Stackoverflow https://stackoverflow.com/a/67361329/11292753\n    )\n    return dataset\n\n# get pretrained VGG16 model using imagenet weights\ndef load_vgg16_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.VGG16(weights='imagenet', input_shape=input_shape)\n    transfer_layer = base_model.get_layer('block5_pool')\n    pretrained_model = Model(inputs = base_model.input, outputs = transfer_layer.output)\n    \n    # consider which layers you want to freeze before you compile \n    for i in range(17):\n        pretrained_model.layers[i].trainable = False\n\n# get pretrained Xception model using imagenet weights\ndef load_xception_pretrained_model(input_shape):\n    \n    base_model = tf.keras.applications.Xception(weights='imagenet',\n                                                include_top=False,\n                                                input_shape=input_shape)\n    \n    \n    # freeze all layers but last 10\n    for i in range(len(base_model.layers) - 10):\n         base_model.layers[i].trainable = False\n    return base_model\n\n#build our model based on pretrained model\ndef make_model(exponential_decay, print_summary = False):\n    \n    input_shape = (224, 224, 3)\n\n    base_model = load_xception_pretrained_model(input_shape)\n    x = base_model.output\n    x = Flatten()(x) #flatten all the output to FC layer\n    x = Dense(512, activation='relu')(x) # Relu Dense of 512 units\n    x = Dropout(0.3)(x) # use Dropout to decrease overfitting\n    x = Dense(256, activation='relu')(x)\n    predictions = Dense(1, activation='sigmoid')(x) # Sigmoid activation: [0,1]\n    model = Model(inputs=base_model.inputs, outputs=predictions)\n        \n    #use exponential decay to have different learning weights for trainable layers from\n    # the pretrained model and ones we added on top.\n    initial_learning_rate = 0.01\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n    )\n\n    loss = 'binary_crossentropy'  #either pneumonia or normal\n    metrics = ['binary_accuracy']\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5 if not exponential_decay else lr_schedule),\n        loss=loss,\n        metrics=metrics,\n    )\n    \n    if print_summary:\n        model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:15:25.990569Z","iopub.execute_input":"2022-08-18T10:15:25.991129Z","iopub.status.idle":"2022-08-18T10:15:26.005190Z","shell.execute_reply.started":"2022-08-18T10:15:25.991085Z","shell.execute_reply":"2022-08-18T10:15:26.004220Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"#create checkboint and earlystopping to use by history\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    \"pneumonia_model.h5\", save_best_only=True, monitor= 'val_loss'\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    patience=100, restore_best_weights=True, monitor= 'val_loss'\n)\n\nmodel = make_model(exponential_decay = True, print_summary=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T09:32:09.945764Z","iopub.execute_input":"2022-08-18T09:32:09.946152Z","iopub.status.idle":"2022-08-18T09:32:11.366962Z","shell.execute_reply.started":"2022-08-18T09:32:09.946119Z","shell.execute_reply":"2022-08-18T09:32:11.366038Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Train our model","metadata":{}},{"cell_type":"code","source":"train_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_train.tfrec\")\nval_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_val.tfrec\")\n\nbatch_size = 64\nepochs = 20\nn_train_images = len(train_normal) + len(train_pneumonia)\nsteps_per_epoch = 10\n\n# From Stackover: tf.data builds a performance model of the input pipeline\n# and runs an optimization algorithm to find a good allocation of its CPU \n# budget across all parameters specified as AUTOTUNE\n# Stackover flow https://stackoverflow.com/a/59493168/11292753\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = get_dataset(train_filenames, batch_size)#n_train_images)\nval_dataset = get_dataset(val_filenames, 8)\n\n# debugging useful\n# image_batch = next(iter(train_dataset))\n# imgs_tensor = tf.constant(image_batch[0])\n# labels_tensor=tf.constant(image_batch[1])\n\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:15:29.890570Z","iopub.execute_input":"2022-08-18T10:15:29.890920Z","iopub.status.idle":"2022-08-18T10:15:51.676018Z","shell.execute_reply.started":"2022-08-18T10:15:29.890890Z","shell.execute_reply":"2022-08-18T10:15:51.675026Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"def plot_history_acc(history, loss_metric_type):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    if loss_metric_type == \"binary\":\n        plt.plot(np.arange(0, N), history.history[\"binary_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_binary_accuracy\"], label=\"val_acc\")\n    else:\n        plt.plot(np.arange(0, N), history.history[\"categorical_accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, N), history.history[\"val_categorical_accuracy\"], label=\"val_acc\")\n        \n    plt.title(\"Training Accuracy on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"lower left\")\n    \n    return\n\ndef plot_history_loss(history):\n    \n    N = len(history.history[\"loss\"])\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n        \n    plt.title(\"Training Loss on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"lower left\")\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2022-08-18T09:56:53.939876Z","iopub.execute_input":"2022-08-18T09:56:53.940678Z","iopub.status.idle":"2022-08-18T09:56:53.953419Z","shell.execute_reply.started":"2022-08-18T09:56:53.940632Z","shell.execute_reply":"2022-08-18T09:56:53.952337Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"#plot epoch-accuracy chart\nloss_metric_type = 'binary' #'binary'\nplot_history_acc(history, loss_metric_type)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:16:30.816084Z","iopub.execute_input":"2022-08-18T10:16:30.816456Z","iopub.status.idle":"2022-08-18T10:16:31.025945Z","shell.execute_reply.started":"2022-08-18T10:16:30.816425Z","shell.execute_reply":"2022-08-18T10:16:31.024938Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"#plot epoch-loss chart\nplot_history_loss(history)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:16:32.235761Z","iopub.execute_input":"2022-08-18T10:16:32.236230Z","iopub.status.idle":"2022-08-18T10:16:32.456007Z","shell.execute_reply.started":"2022-08-18T10:16:32.236190Z","shell.execute_reply":"2022-08-18T10:16:32.454980Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model","metadata":{}},{"cell_type":"code","source":"test_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_test.tfrec\")\n\nn_test_images = len(train_normal) + len(train_pneumonia)\ntest_dataset = get_dataset(test_filenames, batch_size)\n\nmodel.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T10:16:35.828965Z","iopub.execute_input":"2022-08-18T10:16:35.829691Z","iopub.status.idle":"2022-08-18T10:16:40.984191Z","shell.execute_reply.started":"2022-08-18T10:16:35.829655Z","shell.execute_reply":"2022-08-18T10:16:40.983068Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def show_batch_predictions(image_batch):\n    \n    #get rid of Eager Tensor to be able to iterate\n    img_tensor=tf.constant(image_batch[0])\n    labels_tensor=tf.constant(image_batch[1])\n    \n    # debugging: print correct labels: 1= pneumonia, 0 = normal\n    print(\"correct labels:\", labels_tensor)\n    \n    plt.figure(figsize=(14, 14))\n    for n in range(32):\n        ax = plt.subplot(8, 4, n + 1)\n        plt.imshow(img_tensor[n] / 255.0)\n        img_array = tf.expand_dims(img_tensor[n], axis=0)\n        #print( model.predict(img_array)[0])\n        num_predicted_label = model.predict(img_array)[0]\n        \n        # debugging\n        #print(num_predicted_label)\n        \n        # from TF docs:\n        # Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero.\n        # The sigmoid function always returns a value between 0 and 1.\n\n        str_label = \"normal\" if num_predicted_label < 0.5 else \"pneumonia\"\n        correct = \"T\" if num_predicted_label == labels_tensor[n] else \"F\"\n        plt.title(\"{0} - {1}\".format(str_label, correct))\n        plt.axis(\"off\")\n\n\nimage_batch = next(iter(test_dataset))\nshow_batch_predictions(image_batch)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:22:09.093837Z","iopub.execute_input":"2022-08-18T08:22:09.094191Z","iopub.status.idle":"2022-08-18T08:22:14.954717Z","shell.execute_reply.started":"2022-08-18T08:22:09.094153Z","shell.execute_reply":"2022-08-18T08:22:14.953838Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"export_dir= \"export_dir\"\n\nif not os.path.exists(export_dir):\n    os.makedirs(export_dir)  # creating TFRecords output folder\n    print(\"{0} directory created to save exported model\".format(export_dir))\n\nexport_path = path.join(\"export_dir\", \"pneumonia.pb\")\n\ntf.keras.models.save_model(\n    model,\n    export_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None\n)\n\nprint('\\nSaved model')","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:22:14.956185Z","iopub.execute_input":"2022-08-18T08:22:14.957070Z","iopub.status.idle":"2022-08-18T08:22:38.329572Z","shell.execute_reply.started":"2022-08-18T08:22:14.957034Z","shell.execute_reply":"2022-08-18T08:22:38.328504Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Download model","metadata":{}},{"cell_type":"code","source":"# from IPython.display import FileLink\n\n# !zip -r model.zip export_dir\n# FileLink(r'model.zip')","metadata":{"execution":{"iopub.status.busy":"2022-08-18T08:22:38.338080Z","iopub.execute_input":"2022-08-18T08:22:38.338800Z","iopub.status.idle":"2022-08-18T08:23:21.497073Z","shell.execute_reply.started":"2022-08-18T08:22:38.338746Z","shell.execute_reply":"2022-08-18T08:23:21.495905Z"},"trusted":true},"execution_count":20,"outputs":[]}]}